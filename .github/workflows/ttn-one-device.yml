name: TTN all devices → Pages

on:
  schedule:
    - cron: "*/30 * * * *"     # alle 30 Minuten (UTC)
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: ttn-pages
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest

    env:
      TTN_APP_ID:  ${{ secrets.TTN_APP_ID }}
      TTN_REGION:  ${{ secrets.TTN_REGION }}
      TTN_API_KEY: ${{ secrets.TTN_API_KEY }}
      DEBUG_RECENT_MINUTES: "90"      # wird vom Script gelesen
      # TTN_AFTER_DAYS wird unten im Build-Step gesetzt

    steps:
      - name: Checkout (full history)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas plotly pyarrow python-dotenv

      - name: Build dashboard (pull all devices)
        run: |
          set -e
          # größeres Zeitfenster für initialen/fallschrittlichen Pull
          RUN_DASH=1 TTN_AFTER_DAYS=7 python scripts/pull_all_devices.py

          # Nach Build: Assets protokollieren
          ls -l assets/ || true

          # In Pages-Verzeichnis spiegeln
          mkdir -p docs/assets
          cp -r assets/* docs/assets/ || true

      - name: Publish data artifacts to docs/data
        run: |
          mkdir -p docs/data
          cp -a data/*.parquet docs/data/ 2>/dev/null || true
          cp -a data/*.csv     docs/data/ 2>/dev/null || true   # <-- NEU: CSV mitnehmen
          cp -a data/*.ndjson  docs/data/ 2>/dev/null || true

      - name: Build data index (JSON)
        run: |
          python - << 'PY'
          import json, pathlib, time
          d = pathlib.Path("docs/data")
          d.mkdir(parents=True, exist_ok=True)
          items = []
          for p in sorted(d.glob("*")):
              suf = p.suffix.lower()
              if suf not in (".parquet", ".ndjson", ".csv"):  # <-- NEU: CSV berücksichtigen
                  continue
              st = p.stat()
              items.append({
                  "name": p.name,
                  "size": st.st_size,
                  "mtime": int(st.st_mtime),
                  "type": "Parquet" if suf==".parquet" else ("NDJSON" if suf==".ndjson" else "CSV")
              })
          (d / "index.json").write_text(json.dumps(items, ensure_ascii=False), encoding="utf-8")
          PY

      - name: Ensure GitHub Pages flags
        run: |
          mkdir -p docs
          touch docs/.nojekyll

      - name: Commit docs/ and data/
        env:
          GIT_AUTHOR_NAME:  ttn-bot
          GIT_AUTHOR_EMAIL: ttn-bot@example.com
          GIT_COMMITTER_NAME:  ttn-bot
          GIT_COMMITTER_EMAIL: ttn-bot@example.com
        run: |
          set -e
          git add -A
          git commit -m "update all-devices $(date -u +%FT%TZ)" || echo "no changes"
          git push
